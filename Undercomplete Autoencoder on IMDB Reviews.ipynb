{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8c4eae7",
   "metadata": {},
   "source": [
    "# Undercomplete Autoencoder on IMDb Reviews\n",
    "Load the Keras IMDb dataset (top 10,000 words, first 200 tokens per review), build an undercomplete autoencoder to learn a compact representation, and reconstruct the sequences.\n",
    "\n",
    "Pipeline\n",
    "1) Load & pad reviews (top 10k; 200 tokens)\n",
    "2) Sequence autoencoder (Embedding → Encoder LSTM → Code → Repeat → Decoder LSTM → token softmax)\n",
    "3) Sweep small code sizes; pick smallest that reaches a validation-loss target\n",
    "4) Plot training/validation loss\n",
    "5) Show 5 test samples: original vs reconstructed (indices → words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11edd0ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Embedding, LSTM, Dense, TimeDistributed, RepeatVector\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from keras.datasets import imdb\n",
    "from keras.preprocessing import sequence\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print('TensorFlow:', tf.__version__)\n",
    "tf.random.set_seed(123)\n",
    "np.random.seed(123)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dff9412",
   "metadata": {},
   "source": [
    "## 1.Load & Prepare IMDB (Top 10,000 words; first 200 tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2edec6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "max_features = 10000\n",
    "maxlen = 200\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "X_train = pad_sequences(X_train, maxlen=maxlen)\n",
    "X_test  = pad_sequences(X_test,  maxlen=maxlen)\n",
    "\n",
    "print('Train shape:', X_train.shape, 'Test shape:', X_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d21bf9d",
   "metadata": {},
   "source": [
    "## 2.Index-Word Mapping and Text Rendering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd7b7f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = imdb.get_word_index()\n",
    "index_from = 3  \n",
    "reverse_index = { (idx + index_from): word for word, idx in word_index.items() }\n",
    "reverse_index[0] = '[PAD]'\n",
    "reverse_index[1] = '[START]'\n",
    "reverse_index[2] = '[OOV]'\n",
    "\n",
    "def decode_indices(indices, skip_pad=True):\n",
    "    words = []\n",
    "    for idx in indices:\n",
    "        if skip_pad and idx == 0:\n",
    "            continue\n",
    "        words.append(reverse_index.get(int(idx), '[UNK]'))\n",
    "    return ' '.join(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be8034d",
   "metadata": {},
   "source": [
    "## 3.Sequence Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b168719",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = max_features\n",
    "embed_dim  = 64\n",
    "\n",
    "def build_seq_autoencoder(code_dim):\n",
    "    inp = Input(shape=(maxlen,), dtype='int32')\n",
    "    x = Embedding(vocab_size, embed_dim, mask_zero=True, name='embed')(inp)\n",
    "    enc = LSTM(128, return_sequences=False, name='encoder_lstm')(x)\n",
    "    code = Dense(code_dim, activation=None, name='code')(enc)\n",
    "    rep = RepeatVector(maxlen)(code)\n",
    "    dec = LSTM(128, return_sequences=True, name='decoder_lstm')(rep)\n",
    "    logits = TimeDistributed(Dense(vocab_size, activation='softmax'), name='token_logits')(dec)\n",
    "    model = Model(inp, logits, name=f'seq_autoencoder_latent{code_dim}')\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "487cce2f",
   "metadata": {},
   "source": [
    "## 4.Train: Sweep Small Code Sizes and Select the Smallest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e068436",
   "metadata": {},
   "outputs": [],
   "source": [
    "code_candidates = [8, 16, 32, 64]\n",
    "val_loss_threshold = 3.0  \n",
    "\n",
    "histories = {}\n",
    "chosen_model = None\n",
    "chosen_code_dim = None\n",
    "\n",
    "# Targets are original sequences (identity reconstruction)\n",
    "y_train_targets = np.expand_dims(X_train, -1)\n",
    "\n",
    "for cd in code_candidates:\n",
    "    model = build_seq_autoencoder(cd)\n",
    "    es = EarlyStopping(monitor='val_loss', patience=2, restore_best_weights=True)\n",
    "    h = model.fit(\n",
    "        X_train, y_train_targets,\n",
    "        validation_split=0.1,\n",
    "        epochs=9,\n",
    "        batch_size=256,\n",
    "        verbose=1,\n",
    "        callbacks=[es]\n",
    "    )\n",
    "    histories[cd] = h.history\n",
    "    final_val = h.history['val_loss'][-1]\n",
    "    print(f'[code_dim={cd}] final val_loss: {final_val:.4f}')\n",
    "    if final_val <= val_loss_threshold and chosen_model is None:\n",
    "        chosen_model = model\n",
    "        chosen_code_dim = cd\n",
    "        break\n",
    "\n",
    "if chosen_model is None:\n",
    "    chosen_code_dim = min(histories.keys(), key=lambda k: min(histories[k]['val_loss']))\n",
    "    print(f'No code_dim met threshold; using best by val_loss: {chosen_code_dim}')\n",
    "    chosen_model = build_seq_autoencoder(chosen_code_dim)\n",
    "    es = EarlyStopping(monitor='val_loss', patience=2, restore_best_weights=True)\n",
    "    _ = chosen_model.fit(\n",
    "        X_train, np.expand_dims(X_train, -1),\n",
    "        validation_split=0.1,\n",
    "        epochs=9,\n",
    "        batch_size=256,\n",
    "        verbose=1,\n",
    "        callbacks=[es]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f647a7b",
   "metadata": {},
   "source": [
    "## Plot Train/Validation Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e44eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "h = histories.get(chosen_code_dim)\n",
    "if h is not None:\n",
    "    plt.figure()\n",
    "    plt.plot(h['loss'], label='train_loss')\n",
    "    plt.plot(h['val_loss'], label='val_loss')\n",
    "    plt.xlabel('Epoch'); plt.ylabel('Sparse CE'); plt.title(f'Loss (code_dim={chosen_code_dim})')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "else:\n",
    "    print('History not found; model was retrained in fallback.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a27006b",
   "metadata": {},
   "source": [
    "## 5.Five Random Test Samples: Original vs Reconstructed (Text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df6ee879",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.random.choice(len(X_test), size=5, replace=False)\n",
    "samples = X_test[idx]\n",
    "pred = chosen_model.predict(samples, verbose=0)\n",
    "recon_ids = np.argmax(pred, axis=-1).astype('int32')\n",
    "\n",
    "for i in range(5):\n",
    "    print('\\n=== Sample', i+1, '(code_dim =', chosen_code_dim, ') ===')\n",
    "    print('Original :\\n', decode_indices(samples[i], skip_pad=True)[:1000])\n",
    "    print('\\nReconst.:\\n', decode_indices(recon_ids[i], skip_pad=True)[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b037d21f",
   "metadata": {},
   "source": [
    "## Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f3b16ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Smallest number of codings (latent code_dim) selected: {chosen_code_dim}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
